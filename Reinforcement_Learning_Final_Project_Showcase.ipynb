{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iSpgUkEy68G"
      },
      "source": [
        "# Active Object Localization with Deep Reinforcement Learning  \n",
        "### Full Reimplementation of Caicedo & Lazebnik (ICCV 2015)\n",
        "\n",
        "This notebook reproduces the neural agent from:\n",
        "\n",
        "**Caicedo & Lazebnik — Active Object Localization, ICCV 2015**\n",
        "\n",
        "It includes:\n",
        "\n",
        "- Class-specific DQN agent  \n",
        "- CNN feature extraction (AlexNet or ResNet)  \n",
        "- Action history vector  \n",
        "- 9 bounding box transformations  \n",
        "- Paper reward function  \n",
        "- Single-object training  \n",
        "- Episode trajectory visualization  \n",
        "- GIF and MP4 animations  \n",
        "- Heatmaps (episode + dataset-level)  \n",
        "- Quantitative evaluation across VOC 2007  \n",
        "\n",
        "This notebook **does NOT use CLIP**, transformers, or zero-shot models,  \n",
        "to remain faithful to the original paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cykWJFCqy9Bh",
        "outputId": "4523e405-bc44-47d3-e467-0299772ff4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.12/dist-packages (4.9.9)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (2.37.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.7.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from shimmy) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.19.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (0.11.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (11.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (1.4.0)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (0.8.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.13.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (4.2.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (5.29.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (18.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (2.32.4)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (3.2.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets) (2.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (0.8.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree->tensorflow_datasets) (25.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]) (2.19.2)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from simple_parsing->tensorflow_datasets) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.12/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.72.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Downloading stable_baselines3-2.7.1-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.0/188.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shimmy, stable-baselines3\n",
            "Successfully installed shimmy-2.0.0 stable-baselines3-2.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install shimmy stable-baselines3[extra] gymnasium torch torchvision tensorflow_datasets opencv-python imageio matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94P8tbcRy_5U"
      },
      "source": [
        "## Imports & Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfD0z_qSzA3I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "from gymnasium.vector import AsyncVectorEnv\n",
        "\n",
        "from IPython.display import Image as IPyImage\n",
        "from IPython.display import display\n",
        "\n",
        "# For GIF/MP4\n",
        "import imageio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NYClhXtzC1Y"
      },
      "source": [
        "## GPU Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex4ULHiOzFD2"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9_pZFGgzGKl"
      },
      "source": [
        "## Set random seeds for reproducibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXyNXXmTzG3Y"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVMQyubkzIDq"
      },
      "source": [
        "## Create output directories (gifs, mp4s, results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTML_TgKzJ01"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"animations\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44hdbhXbzPd9"
      },
      "source": [
        "# **Section 2 — Dataset: PASCAL VOC 2007 (TFDS)**\n",
        "\n",
        "We load VOC 2007 using TensorFlow Datasets (TFDS).  \n",
        "This gives us:\n",
        "\n",
        "- `voc/2007:train`: Training split  \n",
        "- `voc/2007:validation`: Validation split  \n",
        "- `voc/2007:test`: Test split  \n",
        "\n",
        "The ICCV 2015 paper uses:\n",
        "- **train + validation** for training  \n",
        "- **test** for evaluation  \n",
        "\n",
        "We follow that design here.\n",
        "\n",
        "We also define helper functions to convert TFDS bounding boxes  \n",
        "(from normalized format) to pixel coordinates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y4TiiPwzTUS"
      },
      "outputs": [],
      "source": [
        "# Load VOC 2007 data from TFDS\n",
        "ds_train = tfds.load(\"voc/2007\", split=\"train\", shuffle_files=True)\n",
        "ds_val   = tfds.load(\"voc/2007\", split=\"validation\", shuffle_files=True)\n",
        "ds_test  = tfds.load(\"voc/2007\", split=\"test\", shuffle_files=True)\n",
        "\n",
        "# Convert to list for easier random access\n",
        "ds_train = list(ds_train)\n",
        "ds_val = list(ds_val)\n",
        "ds_test = list(ds_test)\n",
        "\n",
        "print(\"Train size:\", len(ds_train))\n",
        "print(\"Validation size:\", len(ds_val))\n",
        "print(\"Test size:\", len(ds_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_ex0v1bzVZb"
      },
      "source": [
        "## VOC 20-Class Label List (in official order)\n",
        "\n",
        "We hardcode the 20-class list used in the VOC 2007 dataset:\n",
        "\n",
        "1. person  \n",
        "2. bird  \n",
        "3. cat  \n",
        "4. cow  \n",
        "5. dog  \n",
        "6. horse  \n",
        "7. sheep  \n",
        "8. aeroplane  \n",
        "9. bicycle  \n",
        "10. boat  \n",
        "11. bus  \n",
        "12. car  \n",
        "13. motorbike  \n",
        "14. train  \n",
        "15. bottle  \n",
        "16. chair  \n",
        "17. dining table  \n",
        "18. potted plant  \n",
        "19. sofa  \n",
        "20. tv/monitor  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgqwvMrozaOM"
      },
      "outputs": [],
      "source": [
        "VOC_CLASSES = [\n",
        "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
        "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
        "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hUpDi5wzdVn"
      },
      "source": [
        "## Bounding Box conversion utility\n",
        "\n",
        "TFDS provides VOC bounding boxes in **normalized format**:\n",
        "\n",
        "- ymin, xmin, ymax, xmax in range [0, 1]\n",
        "\n",
        "We convert this to pixel coordinates using the image dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsVZmCuuzes7"
      },
      "outputs": [],
      "source": [
        "def tfds_box_to_pixel(bbox, img_shape):\n",
        "    \"\"\"\n",
        "    Convert normalized TFDS VOC bbox to absolute pixel coordinates.\n",
        "\n",
        "    bbox = [ymin, xmin, ymax, xmax]\n",
        "    img_shape = (H, W, 3)\n",
        "    \"\"\"\n",
        "    H, W = img_shape[:2]\n",
        "    ymin, xmin, ymax, xmax = bbox\n",
        "    x1 = int(xmin * W)\n",
        "    y1 = int(ymin * H)\n",
        "    x2 = int(xmax * W)\n",
        "    y2 = int(ymax * H)\n",
        "    return x1, y1, x2, y2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkdsJBhUC_fY"
      },
      "outputs": [],
      "source": [
        "def tfds_to_numpy(sample):\n",
        "    return {\n",
        "        \"image\": np.array(sample[\"image\"]),\n",
        "        \"objects\": {\n",
        "            \"label\": np.array(sample[\"objects\"][\"label\"]),\n",
        "            \"bbox\": np.array(sample[\"objects\"][\"bbox\"]),\n",
        "        }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhIk3XPpzgF_"
      },
      "source": [
        "## Extract all GT boxes for one image (for a specific class)\n",
        "\n",
        "This helps with:\n",
        "- selecting training samples for a given class\n",
        "- evaluating multi-object detection\n",
        "- visualizing ground-truth targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3D2CvqQzhnD"
      },
      "outputs": [],
      "source": [
        "def get_all_gt_boxes_for_class(sample, class_name, img_shape):\n",
        "    H, W = img_shape\n",
        "    gt_boxes = []\n",
        "\n",
        "    for label, bbox in zip(sample[\"objects\"][\"label\"], sample[\"objects\"][\"bbox\"]):\n",
        "        cls = VOC_CLASSES[int(label)]\n",
        "        if cls == class_name:\n",
        "            x1, y1, x2, y2 = tfds_box_to_pixel(bbox, (H, W, 3))\n",
        "            gt_boxes.append(Box(x1, y1, x2, y2))\n",
        "\n",
        "    return gt_boxes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H_7v-WQzi7z"
      },
      "source": [
        "## Quick visualization of a raw VOC sample (for debugging)\n",
        "\n",
        "Useful to verify that bounding boxes and class labels load properly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN7kQJ4OzkI5"
      },
      "outputs": [],
      "source": [
        "def visualize_voc_sample(sample):\n",
        "    img = sample[\"image\"]\n",
        "    H, W = img.shape[:2]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(\"VOC Sample\")\n",
        "\n",
        "    for label, bbox in zip(sample[\"objects\"][\"label\"], sample[\"objects\"][\"bbox\"]):\n",
        "        x1, y1, x2, y2 = tfds_box_to_pixel(bbox, img.shape)\n",
        "        class_name = VOC_CLASSES[int(label)]\n",
        "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                             fill=False, edgecolor='r', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1, class_name, color='yellow')\n",
        "\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cCSGiNkzmYE"
      },
      "outputs": [],
      "source": [
        "visualize_voc_sample(random.choice(ds_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9fOAU8YzxOH"
      },
      "source": [
        "# **Section 3 — CNN Feature Extractor (AlexNet or ResNet)**\n",
        "\n",
        "The original ICCV 2015 paper uses **AlexNet conv5** as the feature extractor.  \n",
        "To maintain faithfulness to the paper, AlexNet is supported.\n",
        "\n",
        "We also allow using **ResNet-50** as an optional modern backbone:\n",
        "\n",
        "- Faster convergence  \n",
        "- Higher quality features  \n",
        "- Better stability in DQN  \n",
        "\n",
        "The RL agent does not backprop through the CNN —  \n",
        "features are used *only* for the state representation.\n",
        "\n",
        "The user may select:\n",
        "\n",
        "```python\n",
        "cnn_type = \"alexnet\"\n",
        "cnn_type = \"resnet50\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p1nUEwyz89S"
      },
      "outputs": [],
      "source": [
        "cnn_type = \"resnet50\"   # you may change this anytime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjeYB4DS0Ajn"
      },
      "source": [
        "## Preprocessing transform for CNN input\n",
        "- Resize crop to 224×224\n",
        "- Convert to tensor\n",
        "- Normalize with ImageNet mean/std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgNzsI3H0B8i"
      },
      "outputs": [],
      "source": [
        "preprocess = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqCe4MhO0DaH"
      },
      "source": [
        "## FeatureExtractor\n",
        "\n",
        "This class:\n",
        "\n",
        "- Loads AlexNet or ResNet-50  \n",
        "- Removes classifier layers  \n",
        "- Extracts features from last convolutional layer  \n",
        "- Flattens the feature map into a 1D vector  \n",
        "\n",
        "Output feature dimension:\n",
        "\n",
        "- AlexNet conv5 → **256 × 6 × 6 = 9216**  \n",
        "- ResNet-50 layer4 → **2048 × 7 × 7 = 100,352**  \n",
        "- ResNet-50 global average pool → **2048** (we will use this one)\n",
        "\n",
        "For RL stability, the preferred choice is:\n",
        "\n",
        "### **ResNet-50 global pooled features (2048-D)**  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXheYqrO0EPH"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, cnn_type=\"resnet50\", device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.cnn_type = cnn_type\n",
        "        self.device = device\n",
        "\n",
        "        if cnn_type == \"alexnet\":\n",
        "            from torchvision.models import alexnet, AlexNet_Weights\n",
        "            model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
        "            self.features = model.features  # conv layers only\n",
        "            self.output_dim = 256 * 6 * 6\n",
        "\n",
        "        elif cnn_type == \"resnet50\":\n",
        "            from torchvision.models import resnet50, ResNet50_Weights\n",
        "            model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "            # all layers except the final FC\n",
        "            self.features = nn.Sequential(*list(model.children())[:-1])\n",
        "            self.output_dim = 2048  # after global average pool\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"cnn_type must be 'alexnet' or 'resnet50'\")\n",
        "\n",
        "        self.to(device)\n",
        "        self.eval()  # we never train CNN\n",
        "\n",
        "    def forward(self, img_crop):\n",
        "        \"\"\"\n",
        "        img_crop: numpy image (H, W, 3)\n",
        "        returns: 1D feature vector (numpy)\n",
        "        \"\"\"\n",
        "       # --- Normalize dtype/shape for ToPILImage ---\n",
        "        if isinstance(img_crop, torch.Tensor):\n",
        "            # [C, H, W] -> [H, W, C], assume in [0,1]\n",
        "            if img_crop.dim() == 3 and img_crop.shape[0] in (1, 3):\n",
        "                img_crop = img_crop.permute(1, 2, 0).cpu().numpy()\n",
        "            else:\n",
        "                img_crop = img_crop.cpu().numpy()\n",
        "\n",
        "        if isinstance(img_crop, np.ndarray):\n",
        "            # If float, convert to uint8 [0,255]\n",
        "            if img_crop.dtype != np.uint8:\n",
        "                arr = img_crop\n",
        "                # heuristic: if max <= 1, assume [0,1] and scale\n",
        "                if arr.size > 0 and arr.max() <= 1.0 + 1e-6:\n",
        "                    arr = arr * 255.0\n",
        "                img_crop = np.clip(arr, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Handle empty crops robustly\n",
        "        if img_crop is None or (isinstance(img_crop, np.ndarray) and img_crop.size == 0):\n",
        "            x = torch.zeros((1, 3, 224, 224), device=self.device)\n",
        "        else:\n",
        "            try:\n",
        "                x = preprocess(img_crop).unsqueeze(0).to(self.device)\n",
        "            except Exception:\n",
        "                # Fallback for any weird shape/dtype\n",
        "                x = torch.zeros((1, 3, 224, 224), device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feats = self.features(x)\n",
        "\n",
        "        feats = feats.view(feats.size(0), -1)\n",
        "        return feats.cpu().numpy().squeeze()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDTcoDu-0Gtt"
      },
      "source": [
        "## Quick test of feature extractor\n",
        "This verifies that CNN → feature vector works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdYv_3Io0IFH"
      },
      "outputs": [],
      "source": [
        "test_img = random.choice(ds_train)[\"image\"]\n",
        "fe = FeatureExtractor(cnn_type=cnn_type, device=device)\n",
        "\n",
        "feat = fe(test_img)\n",
        "print(\"Feature dimension:\", feat.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0DAwsF00Qdp"
      },
      "source": [
        "# **Section 4 — Core RL Components**\n",
        "\n",
        "This section implements the foundational components of the Active Localization agent:\n",
        "\n",
        "### 1. Bounding Box Representation (`Box` class)\n",
        "Handles:\n",
        "- clipping to image boundaries  \n",
        "- integer conversion  \n",
        "- width/height calculations  \n",
        "- movement and resizing  \n",
        "\n",
        "### 2. IoU Computation\n",
        "Intersection over Union for evaluating progress and triggers.\n",
        "\n",
        "### 3. Action Transformations\n",
        "The ICCV 2015 paper uses **9 discrete actions**:\n",
        "\n",
        "1. move left  \n",
        "2. move right  \n",
        "3. move up  \n",
        "4. move down  \n",
        "5. scale bigger  \n",
        "6. scale smaller  \n",
        "7. make fatter (increase width)  \n",
        "8. make taller (increase height)  \n",
        "9. trigger (finalize box)  \n",
        "\n",
        "### 4. Reward Function (From the paper)\n",
        "- +1 if IoU increases  \n",
        "- −1 if IoU decreases  \n",
        "- Trigger:\n",
        "  - +3 if IoU ≥ 0.6  \n",
        "  - −3 otherwise  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvB4XL5E0UNn"
      },
      "outputs": [],
      "source": [
        "class Box:\n",
        "    \"\"\"\n",
        "    Bounding box for the RL agent.\n",
        "    Coordinates stored as absolute pixel coords: (x1, y1, x2, y2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x1, y1, x2, y2):\n",
        "        self.x1 = int(x1)\n",
        "        self.y1 = int(y1)\n",
        "        self.x2 = int(x2)\n",
        "        self.y2 = int(y2)\n",
        "\n",
        "    def as_int(self):\n",
        "        return int(self.x1), int(self.y1), int(self.x2), int(self.y2)\n",
        "\n",
        "    def width(self):\n",
        "        return max(1, self.x2 - self.x1)\n",
        "\n",
        "    def height(self):\n",
        "        return max(1, self.y2 - self.y1)\n",
        "\n",
        "    def copy(self):\n",
        "        return Box(self.x1, self.y1, self.x2, self.y2)\n",
        "\n",
        "    def clip(self, W, H):\n",
        "        \"\"\"Ensure box stays within image boundaries.\"\"\"\n",
        "        self.x1 = np.clip(self.x1, 0, W-1)\n",
        "        self.y1 = np.clip(self.y1, 0, H-1)\n",
        "        self.x2 = np.clip(self.x2, 1, W)\n",
        "        self.y2 = np.clip(self.y2, 1, H)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4A4DYMX0Y4-"
      },
      "outputs": [],
      "source": [
        "def iou(boxA, boxB):\n",
        "    \"\"\"\n",
        "    Compute IoU between two Box objects.\n",
        "    \"\"\"\n",
        "\n",
        "    xA1, yA1, xA2, yA2 = boxA.as_int()\n",
        "    xB1, yB1, xB2, yB2 = boxB.as_int()\n",
        "\n",
        "    inter_x1 = max(xA1, xB1)\n",
        "    inter_y1 = max(yA1, yB1)\n",
        "    inter_x2 = min(xA2, xB2)\n",
        "    inter_y2 = min(yA2, yB2)\n",
        "\n",
        "    inter_w = max(0, inter_x2 - inter_x1)\n",
        "    inter_h = max(0, inter_y2 - inter_y1)\n",
        "    inter_area = inter_w * inter_h\n",
        "\n",
        "    areaA = boxA.width() * boxA.height()\n",
        "    areaB = boxB.width() * boxB.height()\n",
        "\n",
        "    union = areaA + areaB - inter_area + 1e-6\n",
        "\n",
        "    return inter_area / union\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Oz6A8l0a3s"
      },
      "source": [
        "## 9 Actions from Caicedo & Lazebnik (ICCV 2015)\n",
        "\n",
        "We implement:\n",
        "\n",
        "0. move left  \n",
        "1. move right  \n",
        "2. move up  \n",
        "3. move down  \n",
        "4. scale bigger  \n",
        "5. scale smaller  \n",
        "6. increase width (fatter)  \n",
        "7. increase height (taller)  \n",
        "8. trigger (terminate episode)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kymLgEL10cky"
      },
      "outputs": [],
      "source": [
        "def apply_action(box, action, W, H):\n",
        "    \"\"\"\n",
        "    Apply one of the 8 transformation actions to the box.\n",
        "    The 9th action (trigger) is handled in the environment.\n",
        "    \"\"\"\n",
        "    new_box = box.copy()\n",
        "\n",
        "    dx = int(0.2 * box.width())\n",
        "    dy = int(0.2 * box.height())\n",
        "\n",
        "    # 0: left\n",
        "    if action == 0:\n",
        "        new_box.x1 -= dx\n",
        "        new_box.x2 -= dx\n",
        "\n",
        "    # 1: right\n",
        "    elif action == 1:\n",
        "        new_box.x1 += dx\n",
        "        new_box.x2 += dx\n",
        "\n",
        "    # 2: up\n",
        "    elif action == 2:\n",
        "        new_box.y1 -= dy\n",
        "        new_box.y2 -= dy\n",
        "\n",
        "    # 3: down\n",
        "    elif action == 3:\n",
        "        new_box.y1 += dy\n",
        "        new_box.y2 += dy\n",
        "\n",
        "    # 4: scale bigger\n",
        "    elif action == 4:\n",
        "        new_box.x1 -= dx\n",
        "        new_box.y1 -= dy\n",
        "        new_box.x2 += dx\n",
        "        new_box.y2 += dy\n",
        "\n",
        "    # 5: scale smaller\n",
        "    elif action == 5:\n",
        "        new_box.x1 += dx\n",
        "        new_box.y1 += dy\n",
        "        new_box.x2 -= dx\n",
        "        new_box.y2 -= dy\n",
        "\n",
        "    # 6: fatter\n",
        "    elif action == 6:\n",
        "        new_box.x1 -= dx\n",
        "        new_box.x2 += dx\n",
        "\n",
        "    # 7: taller\n",
        "    elif action == 7:\n",
        "        new_box.y1 -= dy\n",
        "        new_box.y2 += dy\n",
        "\n",
        "    # clip to image bounds\n",
        "    new_box.clip(W, H)\n",
        "    return new_box\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7VI5gA0gU1"
      },
      "source": [
        "## Reward Function (Paper-Accurate)\n",
        "\n",
        "Between non-trigger actions:\n",
        "\n",
        "- +1  if IoU(new) > IoU(old)\n",
        "- −1  otherwise\n",
        "\n",
        "For trigger action:\n",
        "\n",
        "- +3  if IoU(new) ≥ 0.6\n",
        "- −3  otherwise\n",
        "\n",
        "This matches the ICCV 2015 methodology exactly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsPmoMQU0qBp"
      },
      "source": [
        "# **Section 5 — RL Environment (TFDSVOCEnv)**\n",
        "\n",
        "This is a faithful reproduction of the environment from:\n",
        "\n",
        "**Caicedo & Lazebnik — Active Object Localization (ICCV 2015)**\n",
        "\n",
        "Key features:\n",
        "\n",
        "- The agent starts with the full image as the initial box.\n",
        "- The agent performs **9 actions**:\n",
        "  - 8 transformations\n",
        "  - 1 trigger\n",
        "- The agent receives:\n",
        "  - CNN features of the cropped region\n",
        "  - A 10-step action history (one-hot)\n",
        "- Training episodes:\n",
        "  - Terminate on trigger or max steps\n",
        "- IoR (Inhibition of Return) is supported for inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAia4ZqM00f4"
      },
      "outputs": [],
      "source": [
        "class TFDSVOCEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Paper-accurate RL environment for VOC object localization.\n",
        "    - State = CNN features + action history\n",
        "    - Reward = IoU-based (from ICCV 2015 paper)\n",
        "    - Actions = 9 discrete actions\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render.modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(self, ds, class_name, feature_extractor, max_steps=40, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ds = ds                       # dataset list\n",
        "        self.class_name = class_name       # target VOC class\n",
        "        self.fe = feature_extractor        # CNN feature extractor\n",
        "        self.device = device\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "        # action history parameters\n",
        "        self.hist_len = 10                 # last 10 actions\n",
        "        self.num_actions = 9               # 8 transforms + trigger\n",
        "        self.hist_dim = self.hist_len * self.num_actions\n",
        "\n",
        "        # Final state dimension: CNN features + history\n",
        "        self.state_dim = self.fe.output_dim + self.hist_dim\n",
        "\n",
        "        # Gym spaces\n",
        "        self.action_space = gym.spaces.Discrete(self.num_actions)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(self.state_dim,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    def reset(self, seed=None, options=None):\n",
        "      super().reset(seed=seed)\n",
        "\n",
        "      while True:\n",
        "          sample = random.choice(self.ds)\n",
        "\n",
        "          # Store the original TFDS sample for potential visualization\n",
        "          self._current_sample = sample\n",
        "\n",
        "          # ---- CRITICAL FIX: convert all TF tensors to numpy ----\n",
        "          img = np.array(sample[\"image\"])\n",
        "          labels = np.array(sample[\"objects\"][\"label\"])\n",
        "          bboxes = np.array(sample[\"objects\"][\"bbox\"])\n",
        "\n",
        "          H, W = img.shape[:2]\n",
        "\n",
        "          boxes = []\n",
        "          for label, bbox in zip(labels, bboxes):\n",
        "              class_name = VOC_CLASSES[int(label)]\n",
        "              if class_name == self.class_name:\n",
        "                  ymin, xmin, ymax, xmax = [float(x) for x in bbox]\n",
        "                  x1 = int(xmin * W)\n",
        "                  y1 = int(ymin * H)\n",
        "                  x2 = int(xmax * W)\n",
        "                  y2 = int(ymax * H)\n",
        "                  boxes.append(Box(x1, y1, x2, y2))\n",
        "\n",
        "          if len(boxes) > 0:\n",
        "              break\n",
        "\n",
        "      # Assign fixed numpy image\n",
        "      self.image = img\n",
        "      self.gt_boxes = boxes\n",
        "      self.gt_box = random.choice(boxes)\n",
        "      self.H, self.W = H, W\n",
        "\n",
        "      self.ior_mask = np.zeros((self.H, self.W), dtype=np.uint8)\n",
        "\n",
        "      self.box = Box(0, 0, self.W, self.H)\n",
        "      self.history = []\n",
        "      self.steps = 0\n",
        "\n",
        "      return self._get_state(), {}\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Apply transformation or trigger.\n",
        "        Paper reward:\n",
        "        - +1 if IoU improves\n",
        "        - -1 otherwise\n",
        "        - Trigger: +3 if IoU >= 0.6, else -3\n",
        "        \"\"\"\n",
        "        old_iou = iou(self.box, self.gt_box)\n",
        "\n",
        "        # Apply transform actions (0-7)\n",
        "        if action != 8:\n",
        "            self.box = apply_action(self.box, action, self.W, self.H)\n",
        "\n",
        "        # Compute new IoU\n",
        "        new_iou = iou(self.box, self.gt_box)\n",
        "\n",
        "        # Trigger (action 8)\n",
        "        if action == 8:\n",
        "            reward = 3 if new_iou >= 0.6 else -3\n",
        "            terminated = True\n",
        "\n",
        "            # IoR update for multi-detection inference\n",
        "            if new_iou >= 0.6:\n",
        "                x1, y1, x2, y2 = self.box.as_int()\n",
        "                self.ior_mask[y1:y2, x1:x2] = 1\n",
        "\n",
        "        else:\n",
        "            # Non-trigger reward\n",
        "            reward = 1 if new_iou > old_iou else -1\n",
        "            terminated = False\n",
        "\n",
        "        self.steps += 1\n",
        "        truncated = self.steps >= self.max_steps\n",
        "\n",
        "        # Update action history\n",
        "        self.history.append(action)\n",
        "        if len(self.history) > self.hist_len:\n",
        "            self.history.pop(0)\n",
        "\n",
        "        return self._get_state(), reward, terminated, truncated, {}\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    def _get_state(self):\n",
        "        \"\"\"\n",
        "        State representation = CNN features + one-hot history.\n",
        "        \"\"\"\n",
        "        x1, y1, x2, y2 = self.box.as_int()\n",
        "\n",
        "        # Crop region corresponding to current box\n",
        "        crop = self.image[y1:y2, x1:x2]\n",
        "        if crop.size == 0:\n",
        "            crop = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "\n",
        "        # Apply IoR mask (in inference only)\n",
        "        crop_mask = self.ior_mask[y1:y2, x1:x2]\n",
        "        if crop_mask.size > 0:\n",
        "            crop = crop.copy()\n",
        "            crop[crop_mask == 1] = crop[crop_mask == 1] * 0.2\n",
        "\n",
        "        # CNN feature extraction\n",
        "        cnn_feat = self.fe(crop)\n",
        "\n",
        "        # Action history one-hot encoding\n",
        "        hist = np.zeros(self.hist_dim)\n",
        "        for i, act in enumerate(self.history):\n",
        "            hist[i * self.num_actions + act] = 1\n",
        "\n",
        "        # Concatenate final state\n",
        "        state = np.concatenate([cnn_feat, hist])\n",
        "        return state.astype(np.float32)\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    def reset_search_only(self):\n",
        "        \"\"\"\n",
        "        Reset the agent's search box without changing the image or GT.\n",
        "        Used for multi-object detection (IoR).\n",
        "        \"\"\"\n",
        "        self.box = Box(0, 0, self.W, self.H)\n",
        "        self.history = []\n",
        "        self.steps = 0\n",
        "        return self._get_state(), {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpEypfZF1CvL"
      },
      "source": [
        "# **Section 6 — Training the DQN Agent**\n",
        "\n",
        "We train a **class-specific DQN** using Stable-Baselines3.\n",
        "\n",
        "Paper methodology:\n",
        "- One DQN per object class\n",
        "- CNN feature extractor is fixed (not trained)\n",
        "- RL agent learns bounding box transformations\n",
        "- Reward is based on improvement in IoU\n",
        "\n",
        "For this demo:\n",
        "- We train for **30,000 steps**\n",
        "- Later, you can increase to 200k–1M steps for full training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Jr9Z7g1KhN"
      },
      "outputs": [],
      "source": [
        "def make_env(class_name, split=\"train\"):\n",
        "    \"\"\"\n",
        "    Factory that creates a single CUBActiveEnv instance for a given class.\n",
        "    Compatible with Stable-Baselines3 VecEnvs.\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        # choose dataset by split\n",
        "        ds = ds_train if split == \"train\" else ds_test\n",
        "\n",
        "        # new FeatureExtractor (already defined earlier)\n",
        "        fe = FeatureExtractor(cnn_type=cnn_type, device=device)\n",
        "\n",
        "        # CUBActiveEnv (already defined earlier)\n",
        "        env = TFDSVOCEnv(\n",
        "            ds=ds,\n",
        "            class_name=class_name,\n",
        "            feature_extractor=fe,\n",
        "            max_steps=40,\n",
        "            device=device,\n",
        "        )\n",
        "        return env\n",
        "\n",
        "    return _init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTRLLiPL1Mmw"
      },
      "outputs": [],
      "source": [
        "def create_training_env(class_name):\n",
        "    env_fn = make_env(class_name, split=\"train\")\n",
        "    vec_env = DummyVecEnv([env_fn])\n",
        "    return vec_env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPmdNb5l1PBW"
      },
      "source": [
        "We use DQN hyperparameters approximating the original setup:\n",
        "\n",
        "- learning_rate = 1e-4  \n",
        "- gamma = 0.9  \n",
        "- exploration_fraction = 0.2  \n",
        "- exploration_final_eps = 0.1  \n",
        "- buffer_size = 50k  \n",
        "- batch_size = 32  \n",
        "\n",
        "These values reproduce the behavior of the ICCV 2015 agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49iXpndP1Q29"
      },
      "outputs": [],
      "source": [
        "CLASS_TO_TRAIN = \"person\"   # choose VOC class\n",
        "\n",
        "env_train = make_env(CLASS_TO_TRAIN)()  # Raw env ONLY\n",
        "model = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    env_train,\n",
        "    learning_rate=1e-4,\n",
        "    buffer_size=50000,\n",
        "    batch_size=32,\n",
        "    gamma=0.9,\n",
        "    exploration_fraction=0.2,\n",
        "    exploration_final_eps=0.1,\n",
        "    verbose=1,\n",
        "    tensorboard_log=\"./tb_logs/\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAaLyqgu1URo"
      },
      "outputs": [],
      "source": [
        "TRAIN_STEPS = 30000  # increase later if needed\n",
        "model.learn(total_timesteps=TRAIN_STEPS)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xdds6Jq8eauM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ord5dKL11Whg"
      },
      "outputs": [],
      "source": [
        "model_path = f\"results/dqn_{CLASS_TO_TRAIN}_{cnn_type}_30k.zip\"\n",
        "model.save(model_path)\n",
        "print(\"Model saved:\", model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc9X89AR1Yfl"
      },
      "outputs": [],
      "source": [
        "model = DQN.load(model_path)\n",
        "print(\"Model loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J71CL0U21Z6V"
      },
      "outputs": [],
      "source": [
        "test_env = make_env(CLASS_TO_TRAIN, split=\"test\")()\n",
        "obs, _ = test_env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, terminated, truncated, info = test_env.step(action)\n",
        "    done = terminated or truncated\n",
        "    total_reward += reward\n",
        "\n",
        "print(\"Test episode reward:\", total_reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4VqUyB01iMH"
      },
      "source": [
        "# **Section 7 — Episode Visualization Tools**\n",
        "\n",
        "These tools allow us to inspect:\n",
        "\n",
        "- How the bounding box moves during an episode  \n",
        "- Whether IoU consistently improves  \n",
        "- How many steps the agent takes  \n",
        "- The quality of the final localization  \n",
        "\n",
        "We provide:\n",
        "\n",
        "### 1. `run_episode_collect_data()`\n",
        "Collects all predicted boxes and IoUs.\n",
        "\n",
        "### 2. `draw_box()`\n",
        "Draws a labeled bounding box onto an image.\n",
        "\n",
        "### 3. `visualize_episode()`\n",
        "Plots:\n",
        "- Agent trajectory  \n",
        "- IoU curve  \n",
        "- Final detection overlay  \n",
        "\n",
        "These visualizations help confirm that training is behaving correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P7FbXj91mPQ"
      },
      "outputs": [],
      "source": [
        "def draw_box(image, box, color=(0,255,0), label=None, thickness=2):\n",
        "    img = image.copy()\n",
        "    x1, y1, x2, y2 = box.as_int()\n",
        "    cv2.rectangle(img, (x1,y1), (x2,y2), color, thickness)\n",
        "\n",
        "    if label is not None:\n",
        "        cv2.putText(img, label, (x1, max(0, y1-5)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hURSJYLk1nw6"
      },
      "outputs": [],
      "source": [
        "def run_episode_collect_data(env, model):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    boxes = []\n",
        "    ious = []\n",
        "    actions = []\n",
        "\n",
        "    # initial box\n",
        "    boxes.append(env.box.copy())\n",
        "    ious.append(iou(env.box, env.gt_box))\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        boxes.append(env.box.copy())\n",
        "        ious.append(iou(env.box, env.gt_box))\n",
        "        actions.append(action)\n",
        "\n",
        "    return boxes, ious, actions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pb6n3vEa1q2H"
      },
      "outputs": [],
      "source": [
        "def visualize_episode(sample, boxes, ious, class_name):\n",
        "    \"\"\"\n",
        "    Visualize:\n",
        "    - GT boxes only for class_name\n",
        "    - Final prediction (if IoU >= 0.6)\n",
        "    \"\"\"\n",
        "    img = sample[\"image\"].copy()\n",
        "    H, W = img.shape[:2]\n",
        "\n",
        "    # ---- Safety: empty ious or boxes ----\n",
        "    if ious is None or len(ious) == 0 or boxes is None or len(boxes) == 0:\n",
        "        final_iou = 0.0\n",
        "        final_box = None\n",
        "    else:\n",
        "        final_box = boxes[-1]\n",
        "        final_iou = ious[-1]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "\n",
        "    # ----- Draw GT only for target class -----\n",
        "    for label, bbox in zip(sample[\"objects\"][\"label\"], sample[\"objects\"][\"bbox\"]):\n",
        "        cls = VOC_CLASSES[int(label)]\n",
        "        if cls == class_name:\n",
        "            x1, y1, x2, y2 = tfds_box_to_pixel(bbox, img.shape)\n",
        "            img = draw_box(img, Box(x1, y1, x2, y2),\n",
        "                           color=(255, 0, 0),\n",
        "                           label=f\"GT: {cls}\")\n",
        "\n",
        "    # ----- Threshold logic for final prediction -----\n",
        "    if final_iou >= 0.6 and final_box is not None:\n",
        "        img = draw_box(\n",
        "            img,\n",
        "            final_box,\n",
        "            color=(0,255,0),\n",
        "            label=f\"Pred: {class_name}, IoU={final_iou:.2f}\"\n",
        "        )\n",
        "        title = f\"Final Prediction: {class_name}\"\n",
        "    else:\n",
        "        title = f\"No {class_name} found (IoU={final_iou:.2f} < 0.6)\"\n",
        "\n",
        "    # ----- Show figure -----\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(title)\n",
        "    ax.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5glnH_sHCc6N"
      },
      "outputs": [],
      "source": [
        "env = make_env(\"person\")()\n",
        "obs, _ = env.reset()\n",
        "\n",
        "print(type(env.image))\n",
        "print(type(obs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIkexqtn1uDk"
      },
      "outputs": [],
      "source": [
        "# Create a fresh test environment\n",
        "env_test = TFDSVOCEnv(\n",
        "    ds=ds_test,\n",
        "    class_name=CLASS_TO_TRAIN,\n",
        "    feature_extractor=FeatureExtractor(cnn_type=cnn_type, device=device),\n",
        "    max_steps=40,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Run one episode. The environment will be reset internally by run_episode_collect_data,\n",
        "# and env_test will retain the state of the episode that just ran.\n",
        "boxes, ious, actions = run_episode_collect_data(env_test, model)\n",
        "\n",
        "# Visualize\n",
        "# Convert the sample stored in env_test to a numpy-safe format for visualization\n",
        "sample_np = tfds_to_numpy(env_test._current_sample)\n",
        "\n",
        "# Visualize\n",
        "visualize_episode(sample_np, boxes, ious, CLASS_TO_TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh_zuAIrEUwl"
      },
      "outputs": [],
      "source": [
        "def sample_contains_class(sample, class_name):\n",
        "    labels = sample[\"objects\"][\"label\"]\n",
        "    for l in labels:\n",
        "        if VOC_CLASSES[int(l)] == class_name:\n",
        "            return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrYMHIoCEWA8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def get_mixed_test_samples(env, class_name, total=10):\n",
        "    positives = []\n",
        "    negatives = []\n",
        "\n",
        "    # First collect positive and negative pools\n",
        "    for i in range(len(env.ds)):\n",
        "        sample = env.ds[i]\n",
        "\n",
        "        if sample_contains_class(sample, class_name):\n",
        "            positives.append(sample)\n",
        "        else:\n",
        "            negatives.append(sample)\n",
        "\n",
        "    # Ensure random mixing\n",
        "    pos_samples = random.sample(positives, k=min(5, len(positives)))\n",
        "    neg_samples = random.sample(negatives, k=min(5, len(negatives)))\n",
        "\n",
        "    combined = pos_samples + neg_samples\n",
        "    random.shuffle(combined)\n",
        "\n",
        "    return combined[:total]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP1Mpk1nEXUa"
      },
      "outputs": [],
      "source": [
        "def evaluate_10_images(env, model, class_name):\n",
        "    samples = get_mixed_test_samples(env, class_name)\n",
        "\n",
        "    for idx, sample in enumerate(samples):\n",
        "        print(f\"\\n============== IMAGE {idx+1} / 10 ==============\")\n",
        "        sample_np = tfds_to_numpy(sample)\n",
        "        # GT status\n",
        "        has_class = sample_contains_class(sample, class_name)\n",
        "        print(f\"GT contains {class_name}: {has_class}\")\n",
        "\n",
        "        # Run RL episode on this sample\n",
        "        boxes, ious, actions = run_episode_collect_data(env, model)\n",
        "\n",
        "        # Visualize\n",
        "        visualize_episode(sample_np, boxes, ious, class_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-oj62tsEeQl"
      },
      "outputs": [],
      "source": [
        "evaluate_10_images(env_test, model, CLASS_TO_TRAIN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN79SLXF14qB"
      },
      "source": [
        "# **Section 8 — GIF + MP4 Animations (Episode Visualization)**\n",
        "\n",
        "This section generates:\n",
        "\n",
        "### • GIF Animation  \n",
        "### • MP4 Video\n",
        "\n",
        "Each frame displays:\n",
        "- Current predicted bounding box (green)\n",
        "- Ground truth box (red)\n",
        "- Step number overlay\n",
        "- Optional IoU value\n",
        "\n",
        "GIFs are ideal for documentation; MP4 is better for presentations and playback.\n",
        "\n",
        "We implement:\n",
        "\n",
        "1. `generate_episode_frames()`\n",
        "2. `save_gif(frames, filename)`\n",
        "3. `save_mp4(frames, filename)`\n",
        "4. `animate_episode(env, model)`\n",
        "\n",
        "All animations are stored under:\n",
        "./animations/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkrZjrCx19T8"
      },
      "outputs": [],
      "source": [
        "def generate_episode_frames(env, model, class_name):\n",
        "    \"\"\"\n",
        "    Run one episode and collect rendered frames.\n",
        "    Returns:\n",
        "      - frames: list of RGB numpy arrays\n",
        "    \"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    step = 0\n",
        "\n",
        "    # save initial frame\n",
        "    frame = draw_box(env.image, env.box, color=(0,255,0),\n",
        "                     label=f\"step {step}\")\n",
        "    # draw GT in red\n",
        "    x1, y1, x2, y2 = env.gt_box.as_int()\n",
        "    frame = draw_box(frame, env.gt_box, color=(255,0,0), label=\"GT\")\n",
        "    frames.append(frame)\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        step += 1\n",
        "\n",
        "        frame = draw_box(env.image, env.box, color=(0,255,0),\n",
        "                         label=f\"step {step}\")\n",
        "        frame = draw_box(frame, env.gt_box, color=(255,0,0), label=\"GT\")\n",
        "        frames.append(frame)\n",
        "\n",
        "    return frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba0Vozj61-zv"
      },
      "outputs": [],
      "source": [
        "def save_gif(frames, filename, fps=4):\n",
        "    \"\"\"\n",
        "    Save episode frames as animated GIF.\n",
        "    \"\"\"\n",
        "    path = os.path.join(\"animations\", filename)\n",
        "    os.makedirs(\"animations\", exist_ok=True)\n",
        "    imageio.mimsave(path, frames, fps=fps)\n",
        "    print(\"Saved GIF:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y81agGin2A3k"
      },
      "outputs": [],
      "source": [
        "def save_mp4(frames, filename, fps=8):\n",
        "    \"\"\"\n",
        "    Save episode frames as MP4 video.\n",
        "    \"\"\"\n",
        "    path = os.path.join(\"animations\", filename)\n",
        "    os.makedirs(\"animations\", exist_ok=True)\n",
        "\n",
        "    height, width, _ = frames[0].shape\n",
        "    writer = cv2.VideoWriter(\n",
        "        path,\n",
        "        cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "        fps,\n",
        "        (width, height)\n",
        "    )\n",
        "\n",
        "    for frame in frames:\n",
        "        writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    writer.release()\n",
        "    print(\"Saved MP4:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvQlfKf_2CLx"
      },
      "outputs": [],
      "source": [
        "def animate_episode(env, model, class_name):\n",
        "    frames = generate_episode_frames(env, model, class_name)\n",
        "\n",
        "    gif_name = f\"episode_{class_name}.gif\"\n",
        "    mp4_name = f\"episode_{class_name}.mp4\"\n",
        "\n",
        "    save_gif(frames, gif_name)\n",
        "    save_mp4(frames, mp4_name)\n",
        "\n",
        "    return frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa7fvR4M2Dv2"
      },
      "outputs": [],
      "source": [
        "env_test = TFDSVOCEnv(\n",
        "    ds=ds_test,\n",
        "    class_name=CLASS_TO_TRAIN,\n",
        "    feature_extractor=FeatureExtractor(cnn_type=cnn_type, device=device),\n",
        "    max_steps=40,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "frames = animate_episode(env_test, model, CLASS_TO_TRAIN)\n",
        "\n",
        "# Display GIF inline\n",
        "IPyImage(filename=f\"animations/episode_{CLASS_TO_TRAIN}.gif\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0usmdMtN3Bfm"
      },
      "source": [
        "# **Section 9 — Attention Heatmaps (Episode + Dataset-Level)**\n",
        "\n",
        "We visualize the agent's attention using heatmaps:\n",
        "\n",
        "### Episode Heatmap\n",
        "- Tracks which image regions the agent cropped throughout an episode.\n",
        "- Each step updates the heatmap by marking the bounding box region.\n",
        "\n",
        "### Dataset Heatmap\n",
        "- Aggregates heatmaps across multiple test images.\n",
        "- Reveals general search patterns for a class.\n",
        "\n",
        "These maps help diagnose agent behavior and verify improved policies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVXgssqh3Elx"
      },
      "outputs": [],
      "source": [
        "def update_heatmap(heatmap, box):\n",
        "    x1, y1, x2, y2 = box.as_int()\n",
        "    heatmap[y1:y2, x1:x2] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ozc4VaU3G8E"
      },
      "outputs": [],
      "source": [
        "def run_episode_with_heatmap(env, model):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    H, W = env.H, env.W\n",
        "    heatmap = np.zeros((H, W), dtype=np.float32)\n",
        "\n",
        "    boxes = []\n",
        "    ious = []\n",
        "\n",
        "    # Record the initial box\n",
        "    boxes.append(env.box.copy())\n",
        "    ious.append(iou(env.box, env.gt_box))\n",
        "    update_heatmap(heatmap, env.box)\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        boxes.append(env.box.copy())\n",
        "        ious.append(iou(env.box, env.gt_box))\n",
        "        update_heatmap(heatmap, env.box)\n",
        "\n",
        "    return boxes, ious, heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyCENuZD_UVF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def compute_heatmap_for_sample(sample, boxes, img_shape):\n",
        "    H, W = img_shape[:2]\n",
        "    heat = np.zeros((H, W), dtype=np.float32)\n",
        "\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = map(int, [box.x1, box.y1, box.x2, box.y2])\n",
        "        heat[y1:y2, x1:x2] += 1  # visitation count\n",
        "\n",
        "    # Normalize 0–1\n",
        "    heat -= heat.min()\n",
        "    if heat.max() > 0:\n",
        "        heat /= heat.max()\n",
        "\n",
        "    return heat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK-9JYnB3IiD"
      },
      "outputs": [],
      "source": [
        "def visualize_heatmap(image, heatmap, title=\"Attention Heatmap\"):\n",
        "    # Normalize to 0–255\n",
        "    h = heatmap.copy()\n",
        "    if h.max() > 0:\n",
        "        h = h / h.max()\n",
        "\n",
        "    h = (h * 255).astype(np.uint8)\n",
        "\n",
        "    # Apply JET colormap\n",
        "    heat = cv2.applyColorMap(h, cv2.COLORMAP_JET)\n",
        "    heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Overlay heatmap on original\n",
        "    overlay = cv2.addWeighted(image, 0.6, heat, 0.4, 0)\n",
        "\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.imshow(overlay)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRmgF5Xg3J5d"
      },
      "outputs": [],
      "source": [
        "# Create test environment\n",
        "env_test = TFDSVOCEnv(\n",
        "    ds=ds_test,\n",
        "    class_name=CLASS_TO_TRAIN,\n",
        "    feature_extractor=FeatureExtractor(cnn_type=cnn_type, device=device),\n",
        "    max_steps=40,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "boxes, ious, heatmap = run_episode_with_heatmap(env_test, model)\n",
        "\n",
        "visualize_heatmap(env_test.image, heatmap, title=f\"Episode Heatmap ({CLASS_TO_TRAIN})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvqs47XI3ZAm"
      },
      "source": [
        "# **Section 10 — Quantitative Evaluation**\n",
        "\n",
        "We evaluate trained DQN agents on VOC 2007 test set.\n",
        "\n",
        "For each object class:\n",
        "\n",
        "1. Sample test images containing that class  \n",
        "2. Run the RL agent until trigger or timeout  \n",
        "3. Record IoU between final predicted box and closest GT box  \n",
        "4. Compute:\n",
        "   - Mean IoU\n",
        "   - Median IoU\n",
        "   - Accuracy@0.5\n",
        "   - Accuracy@0.6\n",
        "   - Accuracy@0.7\n",
        "\n",
        "Finally:\n",
        "- Aggregate results across all 20 VOC classes  \n",
        "- Produce a results pandas DataFrame  \n",
        "- Optionally export to CSV  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RT1Sy3z3ats"
      },
      "outputs": [],
      "source": [
        "def evaluate_single_episode(env, model):\n",
        "    \"\"\"\n",
        "    Runs a single episode and returns:\n",
        "    - predicted box\n",
        "    - best-matching GT box\n",
        "    - IoU value\n",
        "    \"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "    pred_box = env.box\n",
        "    gt_box = env.gt_box\n",
        "    iou_val = iou(pred_box, gt_box)\n",
        "    return pred_box, gt_box, iou_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3jPBrct3dDJ"
      },
      "outputs": [],
      "source": [
        "def evaluate_class(model, class_name, num_samples=50):\n",
        "    ious = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "\n",
        "        # find image containing class\n",
        "        while True:\n",
        "            sample = random.choice(ds_test)\n",
        "            img = sample[\"image\"]\n",
        "            gt_boxes = get_all_gt_boxes_for_class(sample, class_name, img.shape[:2])\n",
        "            if len(gt_boxes) > 0:\n",
        "                break\n",
        "\n",
        "        # build environment for this image\n",
        "        env = TFDSVOCEnv(\n",
        "            ds=[sample],\n",
        "            class_name=class_name,\n",
        "            feature_extractor=FeatureExtractor(cnn_type=cnn_type, device=device),\n",
        "            max_steps=40,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        _, _, iou_val = evaluate_single_episode(env, model)\n",
        "        ious.append(iou_val)\n",
        "\n",
        "    ious = np.array(ious)\n",
        "\n",
        "    results = {\n",
        "        \"class\": class_name,\n",
        "        \"mean_iou\": float(np.mean(ious)),\n",
        "        \"median_iou\": float(np.median(ious)),\n",
        "        \"acc_50\": float(np.mean(ious >= 0.5)),\n",
        "        \"acc_60\": float(np.mean(ious >= 0.6)),\n",
        "        \"acc_70\": float(np.mean(ious >= 0.7)),\n",
        "        \"all_ious\": ious,\n",
        "    }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LofV3byv3fkF"
      },
      "outputs": [],
      "source": [
        "def evaluate_all_classes(model, num_samples=50):\n",
        "    results = []\n",
        "\n",
        "    for cls in VOC_CLASSES:\n",
        "        print(f\"Evaluating: {cls}\")\n",
        "        res = evaluate_class(model, cls, num_samples=num_samples)\n",
        "        results.append(res)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0otgBWX23hGE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def results_to_dataframe(results):\n",
        "    df = pd.DataFrame([{\n",
        "        \"class\": r[\"class\"],\n",
        "        \"mean_iou\": r[\"mean_iou\"],\n",
        "        \"median_iou\": r[\"median_iou\"],\n",
        "        \"acc_50\": r[\"acc_50\"],\n",
        "        \"acc_60\": r[\"acc_60\"],\n",
        "        \"acc_70\": r[\"acc_70\"]\n",
        "    } for r in results])\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyxLtLOZ3iXb"
      },
      "outputs": [],
      "source": [
        "def save_results_csv(df, filename=\"evaluation_results.csv\"):\n",
        "    path = os.path.join(\"results\", filename)\n",
        "    df.to_csv(path, index=False)\n",
        "    print(\"Saved results to:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjObaOzj3jd8"
      },
      "outputs": [],
      "source": [
        "# Evaluate all classes (reduce num_samples if this is too slow)\n",
        "results = evaluate_all_classes(model, num_samples=30)\n",
        "df_results = results_to_dataframe(results)\n",
        "\n",
        "df_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WkbC3V53lEU"
      },
      "outputs": [],
      "source": [
        "save_results_csv(df_results, \"voc2007_dqn_eval.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}